---
id: "005"
feature: "lancedb-batch-optimization"
status: "draft"
created: "2025-12-19"
---

# Specification: LanceDB Batch Optimization

**Author**: PAI

## Problem Statement

Current embedding generation has poor performance on larger tables due to small batch inserts to LanceDB. The current flow inserts embeddings immediately after each Ollama batch (typically small batches based on model constraints), causing:

1. **High I/O overhead**: Many small writes to LanceDB instead of fewer large writes
2. **Poor scaling**: Performance degrades significantly as table size grows
3. **Misleading progress**: Progress counter shows combined Ollama+LanceDB operations, not actual embedding generation rate

## User Stories

### US-1: Faster Embedding Generation
As a user with a large Tana workspace (100k+ nodes), I want embedding generation to complete in reasonable time so that I can use semantic search without waiting hours.

### US-2: Clear Progress Visibility
As a user running `supertag embed generate`, I want to see how many embeddings Ollama has created vs how many have been persisted to LanceDB, so I understand the actual progress and any buffering happening.

## Requirements

### Functional Requirements

#### FR-1: Batch Size Configuration
- LanceDB inserts MUST be batched to 1,000-10,000 embeddings before writing
- Default batch size: 5,000 embeddings
- Configurable via CLI option: `--lance-batch-size <n>`

#### FR-2: Buffered Insert Strategy
- Embeddings from Ollama MUST be buffered in memory until batch threshold is reached
- Buffer MUST be flushed on:
  - Reaching batch threshold
  - Completion of all embeddings
  - Graceful shutdown (if possible)

#### FR-3: Dual Progress Counters
- Progress display MUST show TWO separate counters:
  1. **Ollama embeddings**: Number of vectors generated by the embedding model
  2. **LanceDB persisted**: Number of vectors written to storage
- Example display format:
  ```
  ⏳ 45.2% | Ollama: 45,200 | LanceDB: 40,000 | 125.3/s | ETA: 2m15s
  ```

#### FR-4: Progress Callback Enhancement
- `BatchEmbedResult` and progress callbacks MUST include:
  - `embeddingsGenerated`: count from Ollama
  - `embeddingsPersisted`: count written to LanceDB
  - `bufferSize`: current buffer occupancy

### Non-Functional Requirements

#### NFR-1: Memory Constraints
- Buffer size MUST NOT exceed reasonable memory limits
- With 1024-dimension vectors (4 bytes each), 10k buffer ≈ 40MB
- Should be safe for most systems

#### NFR-2: Data Integrity
- No embeddings may be lost on normal completion
- Buffer MUST be flushed before returning from `embedNodes()`

#### NFR-3: Backward Compatibility
- Existing CLI usage without new flags MUST work unchanged
- Default behavior improves performance without user action

## Scope

### In Scope
- TanaEmbeddingService buffering layer
- Progress display updates in `embed.ts`
- CLI option for batch size configuration
- Unit tests for buffering logic

### Out of Scope
- Changes to resona library (separate spec if needed)
- Parallel Ollama requests (separate optimization)
- Compression of buffered embeddings

## Success Criteria

1. Embedding generation for 100k nodes completes in < 50% of current time
2. LanceDB insert operations reduced by 100x (from per-Ollama-batch to per-5000)
3. Progress display clearly shows Ollama vs LanceDB counts
4. All existing tests pass
5. New tests cover buffering edge cases

## Open Questions

1. **Q**: Does resona support configurable insert batch sizes, or do we need to buffer externally?
   **A**: TBD - need to investigate resona API

2. **Q**: Should buffer be flushed on Ctrl+C signal?
   **A**: Nice to have, not critical for MVP

## Dependencies

- **resona library update REQUIRED** - see `/Users/fischer/work/resona/.specify/specs/lancedb-batch-optimization/spec.md`
- Ollama must be running and accessible

## Implementation Strategy

The core fix is in **resona**, not in the Tana skill:

1. **resona changes** (primary):
   - Add `storeBatchSize` option to `BatchEmbedOptions`
   - Buffer records in memory until threshold reached
   - Add `stored` and `bufferSize` to progress callback

2. **Tana skill changes** (minimal):
   - Pass `storeBatchSize` option to `embedNodes()`
   - Update progress display to show dual counters
   - Add `--lance-batch-size` CLI option

## Assumptions

1. Memory usage of 40-80MB for buffering is acceptable
2. Ollama embedding rate is the bottleneck, not LanceDB inserts (but many small inserts add overhead)
3. User's system has sufficient RAM for buffering
